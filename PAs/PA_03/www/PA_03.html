<style>
    table.table-bordered {
        border:2px solid black;
    }
    table.table-bordered > tbody > tr > td{
        border:2px solid black;
    }
</style>
<h2> PA: Reinforcement Learning </h2>

<h3> Learning Objectives </h3>
<i>After completing this activity, students should be able to:</i>
<ul>
<li>
  Utilize bellman equations and q-learning to build a
  reinforcement learning apporach
</li>
</ul>


<h3>Partners</h3>
This assignment may be completed individually or in pairs.
<b>If you are doing this in pairs, you must notify me at the beginning
of the project.</b>
My expectation for pairs is that both members are
actively involved, and take full responsibility for all aspects
of the project.  In other words, I expect that you are either
sitting or virtually together to work, and not that you are splitting
up tasks to be completed separately.  If both members of the
group are not able to fully explain the code to me, then
this does not meet this expectation.


<h3>Provided Files</h3>
Download this zip archive <a href="PA_mdp/cs444_pa_mdp.zip">cs444_pa_mdp.zip</a> which contains all the
code and supporting files.
Here is a short summary of what is contained within the
zip file:
<table class="table-responsive-sm table-bordered">
    <col style="width:20%">
    <col style="width:80%">
    <tbody>
    <tr>
        <td colspan="2"><b>Files you will edit:</b></td>
    </tr>
    <tr>
        <td>valueIterationAgents.py</td>
        <td>A value iteration agent for solving known MDPs</td>
    </tr>
    <tr>
        <td>(NOT FOR THIS LAB)qlearningAgents.py</td>
        <td>Q-learning agents for Gridfworld and Crawler.</td>
    </tr>
    <tr>
        <td>(NOT FOR THIS LAB) analysis.py</td>
        <td>A place to put your answers.</td>
    </tr>

    <tr>
        <td colspan="2"><b>Files you might want to look at:</b></td>
    </tr>
    <tr>
        <td>mdp.py</td>
        <td>Defines methods on general MDPs.</td>
    </tr>
    <tr>
        <td>learningAgents.py</td>
        <td>Defines the base classes for <i>ValueEstimationAgent</i>
        and <i>QLearningAgent</i>, whch your agents will extend</td>
    </tr>
    <tr>
        <td>util.py</td>
        <td>Utilities, including <i>util.Counter</i>, which is
        particularly useful for Q-learners</td>
    </tr>
    <tr>
        <td>gridworld.py</td>
        <td>The gridworld implementation</td>
    </tr>
    <tr>
        <td>featureExtractors.py</td>
        <td>Classes for extracting features on (state, action) pairs.
        Used for the approximate Q-learning agent (in
        <i>qlearningAgents.py</i></td>
    </tr>

    <tr>
        <td colspan="2"><b>Support files you can ignore:</b></td>
    </tr>
    <tr>
        <td>environment.py</td>
        <td>Abstract class for general reinforcement learning environments.
        Used by <i>gridworld</i></td>
    </tr>
    <tr>
        <td>graphicsGridworldDisplay.py</td>
        <td>Gridworld graphical display</td>
    </tr>
    <tr>
        <td>graphicsUtil.py.py</td>
        <td>Graphic utility</td>
    </tr>
    <tr>
        <td>textGridworldDisplay</td>
        <td>plug-in for the gridworld text interface</td>
    </tr>
    <tr>
        <td>crawler.py</td>
        <td>The crawler code and test harness.  You will run this but do NOT edit.</td>
    </tr>
    <tr>
        <td>graphicsCrawlerDisplay.py</td>
        <td>GUI for the crawler robot</td>
    </tr>
    <tr>
        <td>autograder.py</td>
        <td>Project autograder</td>
    </tr>
    <tr>
        <td>testParser.py</td>
        <td>Parses autograder test and solution files</td>

    </tr>
    </tbody>
</table>

<style>
    code.hlr{font-size:95%;color:#e83e8c;word-break:break-word;border-radius:5px;border:1px solid;}
</style>

<h3> Value Iteration</h3>
Recall that the value iteration state update equation is:
<p>
<img width="400" src="PA_03/bellman2.png">
</p>
Write a value iteration agent in ValueIterationAgent, which has been partially specified for you in
valueIterationAgents.py. Your value iteration agent is an offline planner, not a reinforcement
learning agent, and so the relevant training option is the number of iterations of value iteration it
should run (option -i) in its initial planning phase.
ValueIterationAgent takes an MDP on construction and runs value iteration for the specified
number of iterations before the constructor returns.

Value iteration computes k-step estimates of the optimal values,V-k. In addition to running
value iteration, implement the following methods for ValueIterationAgent using Vk.
<ul>
    <li>computeActionFromValues(state) computes the best action according to the value function
        given by self.values.
    </li>
    <li>computeQValueFromValues(state, action) returns the Q-value of the (state, action)
        pair given by the value function given by self.values.
    </li>
</ul>

These quantities are all displayed in the GUI: values are numbers in squares, Q-values are numbers in
square quarters, and policies are arrows out from each square.


<h3>Submission and Policies</h3>

You must submit the following files:
<ul>
    <li><code class="hlr">ValueIterationAgents.py</code> to Canvas .
    </li>
</ul>

<p>
    <b>Evaluation:</b>
    Your code will be autograded for technical correctness. Please do not change the names of any
    provided functions or classes within the code, or you will wreak havoc on the autograder. However,
    the correctness of your implementation – not the autograder’s judgements –
    will be the final judge of your score. If necessary, I will review and grade assignments individually
    to ensure that you receive due credit for your work.
</p>
<p>
<b>Academic Dishonesty</b>: Your code will be checked
    against other submissions in the class for logical redundancy. If you copy someone else’s code and submit it with minor changes,
    it will be detected. These cheat detectors are quite hard to fool, so please don’t try.
    I trust you all to submit your own team's work; please don’t let us down.
</p>
<p>
    <b>Getting Help:</b> You are not alone! If you find yourself stuck on something, contact me.
    Office hours and Piazza discussion forum are there for your support; please use them.
    If you can’t make office hours, let me know and I will schedule a meeting with you. I want these projects to be
    rewarding and instructional, not frustrating and demoralizing.
    But, I don’t know when or how to help unless you ask.
</p>


<hr/>

<h2>Project Overview</h2>
<div style="padding-left:2em;">
    <p>
    <h3> MDPs</h3>
    <div style="padding-left:2em;">
        <p>
            You will implement the <b>value iteration</b> algorithm and
            test it in the gridworld setting discussed in class.  For part 1,
            you will need to modify <code class="hlr">valueIterationAgents.py</code> and
            implement the following methods:
            <ul>
                <li>runValueIteration.  This method is
                called from the constructor.  You should use the
                computeQValuesFromValues in this method.</li>
                <li>computeQValueFromValues</li>
                <li>computeActionFromValues</li>
            </ul>

            Recall that value iteration state update equation is:
              <p>
                  <img src="PA_03/bellman2.png" alt="Bellman2"
                       class="center-image img-fluid" style="width: 400px;" />
              </p>
            More information is available on these equations from
            the textbook (Chapter 17) and the class slides.  When implementing
            value iteration, you are only being asked to run for the number
            of iterations (so, do not run until convergence).  You should also
            look at the __init__ method, as it shows some VERY useful
            methods within the self.mdp object that you will need for these
            methods.
            For this part, it is an offline planener (does not use RL).
            The number of iterations to run value iteration is an argument (specified via the
            <code class="hlr">-i</code> command option) and stored within self.iteration
            in the ValueIterationAgent class.  The following descriptions
        are provided for V<sub>k</sub>.

            <p>
                <br/>
                <i>Important:</i> Use the “batch” version of value iteration where each vector Vk is computed from
                a fixed vector V(k-1) (like in lecture), not the “online” version where one single weight
                vector is updated in place. This means that when a state’s value is updated in iteration
                k based on the values of its successor states, the successor state values used in the value
                update computation should be those from iteration k − 1
                (even if some of the successor states had already been updated in iteration k
                ).

                Note: A policy synthesized from values of depth k (which reflect the next k rewards) will actually reflect
                the next k + 1 rewards (i.e. you return π(k+1).
                Similarly, the Q-values will also reflect one more reward than the values (i.e. you return
                Q(k+1).

                You should return the synthesized policy π(k+1).

                Hint: You may optionally use the util.Counter class in util.py, which is a dictionary with a default value of zero. However, be careful with argMax: the actual argmax you want may be a key not in the counter!

                Note: Make sure to handle the case when a state has no available actions
                in an MDP (think about what this means for future rewards).

                To test your implementation, run the autograder:
        <pre><code>python gridworld.py -a value -i 100 -k 10</code></pre>

        The following command loads your ValueIterationAgent, which will compute a policy and execute it
        10 times. Press a key to cycle through values, Q-values, and the simulation. You should find that
        the value of the start state (V(start), which you can read off of the GUI) and the empirical
        resulting average reward (printed after the 10 rounds of execution finish) are quite close.
        <pre><code>python autograder.py -q q1</code></pre>

        Hint:  On the default grid, running value iterations for 5 iterations should give you this output:
        <pre><code>python gridworld.py -a value -i 5</code></pre>
        </p>
        <p>
            <img width="400" src="PA_03/mdp_gridWorldAfter5.png">
        </p>
    </div>
        <p>
        </p>
    </div>
    <p>
        Submissions will be graded according to the following rubric:
    </p>
    <div class="row">
        <div class="col-sm-6">
    <table class="table table-responsive-sm  table-bordered">
        <tr>
            <td>Passes Unit Tests</td><td> 90%</td>
        </tr>

        <tr>
            <td>Coding style</td>
            <td>10% </td>
        </tr>

    </table>
    </div>
    </div>

</div>




<div style="padding-left:2em;">
<small>
    This assignment was developed at the UC-Berkeley by Dan Klein and John
    DeNero with autograding support from  Brad Miller, Nick Hay, and
    Pieter Abbeel.
</small>